---
title: "R markdown file"
author: "Floris Meijvis en co"
date: "2024-11-22"
output: pdf_document
---

Importing packages:
```{r} 
library(ggplot2)
library(tidyverse)
library(spls)
library(tensorflow)
library(caret)
```

# Importing and Mutating Data

-> problem?! gene names are gone in Data dataframe

```{r}
# import and mutate Data
data(prostate)
x <- prostate$x
y <- prostate$y

Data <- data.frame(y, x) %>% 
  # rename column y to tumor
  rename(disease = y)

# explore data
nrow(Data) #102 samples -> 52 tumor samples, 50 normal tissue samples
ncol(Data) #6034 genes included in data set
all(sapply(Data, is.numeric)) # TRUE, all variables are operationalized as numeric
head(Data, 10)
```

The prostate dataset consists of 52 prostate tumor and 50 normal samples. Normal and tumor classes are coded in 0 and 1, respectively, in $Y$ vector. Matrix $X$ is gene expression data and arrays were normalized, log transformed, and standardized to zero mean and unit variance across genes as described in Dettling (2004) and Dettling and Beuhlmann (2002). See Chung and Keles (2010) for more details.

https://rdrr.io/cran/spls/man/prostate.html

# Data visualization

```{r}
### making a few visualizations of the data

# boxplot for possible tumor promoting gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X54, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X54", title = "Expression level of gene X54 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))


# boxplot for possible tumor supressing gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X2022, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X2022", title = "Expression level of gene X2022 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))
```

A few quickly made observations: it seems that the tumors have less variance in the gene expressions (but this is based only on 3 plots, so could be wrong)

```{r}
# compute the average sd in gene expressions for normal and tumor samples
normal <- Data %>% filter(disease == 0) %>% dplyr::select(-disease)
tumor <- Data %>% filter(disease == 1) %>% dplyr::select(-disease)
normal_sd <- apply(normal, 2, sd)
tumor_sd <- apply(tumor, 2, sd)

data_sd <- data.frame(normal = normal_sd, tumor = tumor_sd) %>%
  pivot_longer(cols = everything(), names_to = "disease", values_to = "sd")

# visualize the average sd in gene expressions for normal and tumor samples using boxplot
ggplot(data_sd, aes(x = disease, y = sd, fill = disease)) +
  geom_boxplot() +
  labs(y = "Average standard deviation in gene expressions", title = "Average standard deviation in gene expressions for normal and tumor samples")
```

There does not seem to be massive variation in the standard deviations of gene expressions between normal and tumor samples. However, we do generally see that the standard deviations are lower in the tumor samples compared to the normal samples, especially since there are less high outliers for the tumor samples.

# Method 1: Hybrid step-wise Logistic regression

```{r}
## TRAIN / TEST DATA SPLIT
# divide data in train/test data split (80/20%)

# function ensures that the proportion of the outcome variable is preserved in both sets.
set.seed(123)
training.samples <- Data$disease %>%
  createDataPartition(p = 0.8, list = FALSE) 
train.data <- Data[training.samples, ]
test.data <- Data[-training.samples, ]
dim(train.data)
dim(test.data)

# # Optional: Automatically add quadratic terms for all predictors
# predictors <- colnames(train.data)[-which(colnames(train.data) == "disease")]
# quadratic_data <- train.data
# for (pred in predictors) {
#   quadratic_data[[paste0(pred, "_sq")]] <- train.data[[pred]]^2
# }
# 
# # Update the dataset
# train.data <- quadratic_data

# fit logistic regression models
intercept.model <- glm(disease ~ 1, family = "binomial", data = train.data)
full.model <- glm(disease ~ ., family = "binomial", data = train.data) 
summary(full.model)

# probit.model <- glm(disease ~ ., family = binomial(link = "probit"), data = train.data) # no parameter significant
# summary(probit.model)
# cloglog.model <- glm(disease ~ ., family = binomial(link = "cloglog"), data = train.data) # no parameter significant
# summary(cloglog.model)
# identity.model <- glm(disease ~ ., family = binomial(link = "identity"), data = train.data) # "no valid set of coefficients has been found: please supply starting values"
# there is a clear issue of multicollinearity 
# warning "5952 not defined because of singularities"

# Perform stepwise logistic regression
library(MASS)
step.model <- stepAIC(object = intercept.model, direction = "both", 
                      scope = full.model, trace = FALSE)

# step.model2 <- stepAIC(object = intercept.model, direction = "backward", 
#                       scope = full.model, trace = FALSE)
# 
# step.model3 <- stepAIC(object = intercept.model, direction = "forward", 
#                       scope = full.model, trace = FALSE)

# Predict probabilities and classify on validation set
predictions <- predict(step.model, newdata = test.data, type = "response") # all predictions are identical as the intercept is the only parameter in the final model
pred_class <- ifelse(predictions > 0.5, 1, 0) # and since the intercept is slightly greater then 0.5, all predictions are 1

# Compute confusion matrix
cm <- confusionMatrix(factor(pred_class), factor(test.data$disease), positive = "1")
cm
  
# Compute AUC
roc_curve <- roc(factor(test.data$disease), predictions)
auc(roc_curve)

# draw ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC curve for logistic regression model")
```
Let's now do it with 5-fold cross-validation

```{r}
# Load required libraries
library(caret)
library(pROC)

# Define 5-fold cross-validation
set.seed(123)
folds <- createFolds(train.data$disease, k = 5, list = TRUE, returnTrain = TRUE)

# Initialize vectors to store performance metrics
accuracy <- c()
sensitivity <- c()
specificity <- c()
auc <- c()

# Perform 5-fold CV
for (i in 1:5) {
  # Split the data into training and validation sets
  fold_train <- train.data[folds[[i]], ]
  fold_test <- train.data[-folds[[i]], ]
  
  # Fit intercept and full models
  intercept.model <- glm(disease ~ 1, family = "binomial", data = fold_train)
  full.model <- glm(disease ~ ., family = "binomial", data = fold_train)
  
  # Perform stepwise logistic regression
  step.model <- stepAIC(intercept.model, direction = "both", 
                        scope = full.model, trace = FALSE)
  
  # Predict probabilities and classify on validation set
  predictions <- predict(step.model, newdata = fold_test, type = "response")
  pred_class <- ifelse(predictions > 0.5, 1, 0)
  
  # Compute confusion matrix
  cm <- confusionMatrix(factor(pred_class), factor(fold_test$disease), positive = "1")
  
  # Compute metrics
  accuracy[i] <- cm$overall["Accuracy"]
  sensitivity[i] <- cm$byClass["Sensitivity"]
  specificity[i] <- cm$byClass["Specificity"]
  
  # Compute AUC
  roc_curve <- roc(factor(fold_test$disease), predictions)
  auc[i] <- auc(roc_curve)
}

# Print the results
cat("5-Fold Cross-Validation Results:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("AUC:", auc, "\n")

# the accuracy is between 0.375 and 0.588, which is not very good
# either the sensitivity or specificity is 1, there is no in-between, indicating that the model
# is either good at predicting the positive class or the negative class, but not both.
# the AUC is consistent at 0.5, which is the same as random guessing

# Calculate average metrics
avg_accuracy <- mean(accuracy)
avg_sensitivity <- mean(sensitivity)
avg_specificity <- mean(specificity)
avg_auc <- mean(auc)
```

# Method 2: Principle component analysis (PCA) with logistic regression

what is selection strategy?

-   components explaining a cumulative variance of 95% were retained and used as predictors in logistic regression
-   scree plot?

```{r}
library(pls)
?pcr()

# remove the outcome
Data_pca <- Data %>% dplyr::select(-disease)
# summary(Data_pca[,1:30])
Data_pca <- as.matrix(Data_pca)

Data.pca = prcomp(Data_pca,
                  center = FALSE, # arrays were already normalized and centered
                  scale = FALSE)

Data.pca.stand = prcomp(Data_pca,
                  center = TRUE, # sensitivity analysis
                  scale = TRUE)

# evaluate cumulative variance
summary(Data.pca) 
which(cumsum(Data.pca$sdev^2) / sum(Data.pca$sdev^2) >= 0.95)[1] # 33 components are needed to explain 95% of the variance
summary(Data.pca.stand) 
which(cumsum(Data.pca.stand$sdev^2) / sum(Data.pca.stand$sdev^2) >= 0.95)[1] # 67 components are needed to explain 95% of the variance
# either lower threshold (e.g., 0.90 or use scree plot to determine number of components)

# make scree plot
screeplot(Data.pca, type = "lines", main = "Scree plot of PCA") # two components before the elbow
screeplot(Data.pca.stand, type = "lines", main = "Scree plot of PCA standardized") # one component before the elbow

### Continue with unstandardized PCA and 2 components according to scree plot

# predi
pcs <- predict(Data.pca, newdata = Data_pca, 2)[, 1:2]

# fit logistic regression model
model <- glm(as.factor(Data$disease) ~ ., family = "binomial", data = data.frame(disease = Data$disease, train_pcs))
model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)


```

Let's now implement the 5-fold cross-validation

```{r}

library(caret)     # For cross-validation
library(pROC)      # For AUC calculation
library(dplyr)     # For data manipulation

# Step 1: Set up cross-validation
set.seed(123)
folds <- createFolds(Data$disease, k = 5, list = TRUE)  # 5-fold CV

# Step 2: Cross-validation loop with PCA on training data
results <- data.frame(Components = numeric(),
                      Accuracy = numeric(),
                      AUC = numeric())

components_to_test <- seq(2, 33)  # Test up to 50 components (or adjust based on your data)

for (num_components in components_to_test) {
  metrics <- data.frame(Accuracy = numeric(), AUC = numeric())
  
  for (i in seq_along(folds)) {
    # Split data into training and testing folds
    train_idx <- unlist(folds[-i])
    test_idx <- folds[[i]]
    train_data <- Data[train_idx, ]
    test_data <- Data[test_idx, ]
    
    # Step 3: Perform PCA on training data
    train_pca <- train_data %>% dplyr::select(-disease) %>% as.matrix()
    test_pca <- test_data %>% dplyr::select(-disease) %>% as.matrix()
    
    pca_model <- prcomp(train_pca, center = FALSE, scale = FALSE)  # Fit PCA on training data
    
    # Step 4: Transform both training and testing data using the same PCA model 
    train_pcs <- pca_model$x[, 1:num_components]  # Select top N components for training data
    test_pcs <- predict(pca_model, newdata = test_pca)[, 1:num_components]  # Transform test data
    
    # Combine PCs with the outcome variable
    train_df <- data.frame(disease = train_data$disease, train_pcs)
    test_df <- data.frame(disease = test_data$disease, test_pcs)
    
    # Step 5: Fit logistic regression
    model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)
    
    # Step 6: Predict on the test set
    pred_probs <- predict(model, newdata = test_df, type = "response")
    pred_class <- ifelse(pred_probs > 0.5, 1, 0)
    
    # Calculate metrics
    accuracy <- mean(pred_class == test_df$disease)
    auc <- roc(as.factor(test_df$disease), pred_probs)$auc
    
    # Store metrics for the fold
    metrics <- rbind(metrics, data.frame(Accuracy = accuracy, AUC = auc))
  }
  
  # Average metrics across folds
  avg_accuracy <- mean(metrics$Accuracy)
  avg_auc <- mean(metrics$AUC)
  
  # Store results
  results <- rbind(results, data.frame(Components = num_components,
                                       Accuracy = avg_accuracy,
                                       AUC = avg_auc))
}

# Step 7: Find the optimal number of components
optimal_components <- results %>% 
  filter(AUC == max(AUC)) %>% 
  select(Components) %>% 
  pull()

# Step 8: Visualize results
library(ggplot2)

ggplot(results, aes(x = Components)) +
  geom_line(aes(y = AUC, color = "AUC")) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  labs(title = "5-Fold Cross-Validation Results",
       x = "Number of Principal Components",
       y = "Metric") +
  scale_color_manual(values = c("AUC" = "blue", "Accuracy" = "red")) +
  theme_minimal()

# Print optimal number of components
cat("Optimal number of components:", optimal_components, "\n")

# save results to rds
saveRDS(results, "results_pca.rds")
```


# Method 3: Sparse PCA with logistic regression




---
title: "R markdown file"
author: "Floris Meijvis en co"
date: "2024-11-22"
output: pdf_document
---

Importing packages:

```{r} 
library(ggplot2)
library(tidyverse)
library(spls)
library(tensorflow)
library(caret)
library(caret)
library(pROC)
library(pls)
library(caret)     # For cross-validation
library(pROC)      # For AUC calculation
library(dplyr)     # For data manipulation
```

# Data Pre-processing and Exploration

-> problem?! gene names are gone in Data dataframe

```{r}
# import and mutate Data
data(prostate)
x <- prostate$x
y <- prostate$y

Data <- data.frame(y, x) %>% 
  # rename column y to tumor
  rename(disease = y)

# explore data
nrow(Data) #102 samples -> 52 tumor samples, 50 normal tissue samples
ncol(Data) #6034 genes included in data set
all(sapply(Data, is.numeric)) # TRUE, all variables are operationalized as numeric
head(Data, 10)
```

The prostate dataset consists of 52 prostate tumor and 50 normal samples. Normal and tumor classes are coded in 0 and 1, respectively, in $Y$ vector. Matrix $X$ is gene expression data and arrays were normalized, log transformed, and standardized to zero mean and unit variance across genes as described in Dettling (2004) and Dettling and Beuhlmann (2002). See Chung and Keles (2010) for more details.

https://rdrr.io/cran/spls/man/prostate.html

# Data visualization

```{r}
### making a few visualizations of the data

# boxplot for possible tumor promoting gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X54, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X54", title = "Expression level of gene X54 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))


# boxplot for possible tumor supressing gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X2022, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X2022", title = "Expression level of gene X2022 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))
```

A few quickly made observations: it seems that the tumors have less variance in the gene expressions (but this is based only on 3 plots, so could be wrong)

```{r}
# compute the average sd in gene expressions for normal and tumor samples
normal <- Data %>% filter(disease == 0) %>% dplyr::select(-disease)
tumor <- Data %>% filter(disease == 1) %>% dplyr::select(-disease)
normal_sd <- apply(normal, 2, sd)
tumor_sd <- apply(tumor, 2, sd)

data_sd <- data.frame(normal = normal_sd, tumor = tumor_sd) %>%
  pivot_longer(cols = everything(), names_to = "disease", values_to = "sd")

# visualize the average sd in gene expressions for normal and tumor samples using boxplot
ggplot(data_sd, aes(x = disease, y = sd, fill = disease)) +
  geom_boxplot() +
  labs(y = "Average standard deviation in gene expressions", title = "Average standard deviation in gene expressions for normal and tumor samples")
```

There does not seem to be massive variation in the standard deviations of gene expressions between normal and tumor samples. However, we do generally see that the standard deviations are lower in the tumor samples compared to the normal samples, especially since there are less high outliers for the tumor samples.

# Method 1: Hybrid step-wise Logistic regression

The first approach employed hybrid stepwise logistic regression, implemented using the \texttt{MASS} package’s \texttt{stepAIC()} function. This method iteratively selects predictors by minimizing the Akaike Information Criterion (AIC) through a combination of forward selection and backward elimination \cite{hastie2009elements}. Alternatively, a more rigorous but computationally expensive strategy was also considered: removing one variable at a time and performing an analysis of deviance using differences in residual deviance to identify the least important variable to exclude \cite{hastie2009elements}. 

```{r}
## TRAIN / TEST DATA SPLIT
# divide data in train/test data split (80/20%)

# function ensures that the proportion of the outcome variable is preserved in both sets.
set.seed(123)
training.samples <- Data$disease %>%
  createDataPartition(p = 0.8, list = FALSE) 
train.data <- Data[training.samples, ]
test.data <- Data[-training.samples, ]
dim(train.data)
dim(test.data)

# # Optional: Automatically add quadratic terms for all predictors
# predictors <- colnames(train.data)[-which(colnames(train.data) == "disease")]
# quadratic_data <- train.data
# for (pred in predictors) {
#   quadratic_data[[paste0(pred, "_sq")]] <- train.data[[pred]]^2
# }
# 
# # Update the dataset
# train.data <- quadratic_data

# fit logistic regression models
intercept.model <- glm(disease ~ 1, family = "binomial", data = train.data)
full.model <- glm(disease ~ ., family = "binomial", data = train.data) 
summary(full.model)

# probit.model <- glm(disease ~ ., family = binomial(link = "probit"), data = train.data) # no parameter significant
# summary(probit.model)
# cloglog.model <- glm(disease ~ ., family = binomial(link = "cloglog"), data = train.data) # no parameter significant
# summary(cloglog.model)
# identity.model <- glm(disease ~ ., family = binomial(link = "identity"), data = train.data) # "no valid set of coefficients has been found: please supply starting values"
# there is a clear issue of multicollinearity 
# warning "5952 not defined because of singularities"

# Perform stepwise logistic regression
library(MASS)
step.model <- stepAIC(object = intercept.model, direction = "both", 
                      scope = full.model, trace = FALSE)

# step.model2 <- stepAIC(object = intercept.model, direction = "backward", 
#                       scope = full.model, trace = FALSE)
# 
# step.model3 <- stepAIC(object = intercept.model, direction = "forward", 
#                       scope = full.model, trace = FALSE)

# Predict probabilities and classify on validation set
predictions <- predict(step.model, newdata = test.data, type = "response") # all predictions are identical as the intercept is the only parameter in the final model
pred_class <- ifelse(predictions > 0.5, 1, 0) # and since the intercept is slightly greater then 0.5, all predictions are 1

# Compute confusion matrix
cm <- confusionMatrix(factor(pred_class), factor(test.data$disease), positive = "1")
cm
  
# Compute AUC
roc_curve <- roc(factor(test.data$disease), predictions)
auc(roc_curve)

# draw ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC curve for logistic regression model")
```

Let's now do it with 5-fold cross-validation

```{r}
# Define 5-fold cross-validation
set.seed(123)
folds <- createFolds(train.data$disease, k = 5, list = TRUE, returnTrain = TRUE)

# Initialize vectors to store performance metrics
accuracy <- c()
sensitivity <- c()
specificity <- c()
auc <- c()

# Perform 5-fold CV
for (i in 1:5) {
  # Split the data into training and validation sets
  fold_train <- train.data[folds[[i]], ]
  fold_test <- train.data[-folds[[i]], ]
  
  # Fit intercept and full models
  intercept.model <- glm(disease ~ 1, family = "binomial", data = fold_train)
  full.model <- glm(disease ~ ., family = "binomial", data = fold_train)
  
  # Perform stepwise logistic regression
  step.model <- stepAIC(intercept.model, direction = "both", 
                        scope = full.model, trace = FALSE)
  
  # Predict probabilities and classify on validation set
  predictions <- predict(step.model, newdata = fold_test, type = "response")
  pred_class <- ifelse(predictions > 0.5, 1, 0)
  
  # Compute confusion matrix
  cm <- confusionMatrix(factor(pred_class), factor(fold_test$disease), positive = "1")
  
  # Compute metrics
  accuracy[i] <- cm$overall["Accuracy"]
  sensitivity[i] <- cm$byClass["Sensitivity"]
  specificity[i] <- cm$byClass["Specificity"]
  
  # Compute AUC
  roc_curve <- roc(factor(fold_test$disease), predictions)
  auc[i] <- auc(roc_curve)
}

# Print the results
cat("5-Fold Cross-Validation Results:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("AUC:", auc, "\n")

# the accuracy is between 0.375 and 0.588, which is not very good
# either the sensitivity or specificity is 1, there is no in-between, indicating that the model
# is either good at predicting the positive class or the negative class, but not both.
# the AUC is consistent at 0.5, which is the same as random guessing

# Calculate average metrics
avg_accuracy <- mean(accuracy)
avg_sensitivity <- mean(sensitivity)
avg_specificity <- mean(specificity)
avg_auc <- mean(auc)
```

# Method 2: Principle component analysis (PCA) with logistic regression

The second approach utilized Principal Component Analysis (PCA), performed with the \texttt{stats} package’s \texttt{prcomp()} function. PCA transforms the original 6,024 predictors into orthogonal components that capture the majority of variance in the dataset. In line with the recommendation for PC regression, the number of components was chosen by cross-validation (ISLR, p. 259). More specifically, the number of components was selected based on the predictive performance of logistic regression models in 5-fold cross-validation. Note, While PCA effectively reduces dimensionality, it does not enforce sparsity, as it retains all original features in the transformed components \cite{hastie2015statistical}, (ISLR, p. 259). Further centering or scaling of genes was avoided to preserve sample-level variability and prevent distortion of the gene variance structure, which is critical for PCA. Array-level standardization already ensured comparable feature scales, aligning with the assumptions of sparse PCA (ISLR, p. 259). 

> Make sure that existing pre-processing done to the data was already explained.

```{r}
# remove the outcome
Data_pca <- Data %>% dplyr::select(-disease)

# summary(Data_pca[,1:30])
Data_pca <- as.matrix(Data_pca)

Data.pca = prcomp(Data_pca,
                  center = FALSE, # arrays were already normalized and centered
                  scale = FALSE)

# evaluate cumulative variance
summary(Data.pca) 
which(cumsum(Data.pca$sdev^2) / sum(Data.pca$sdev^2) >= 0.95)[1] 
# 33 components are needed to explain 95% of the variance
# either lower threshold (e.g., 0.90 or use scree plot to determine number of components)

# make scree plot
screeplot(Data.pca, type = "lines", main = "Scree plot of PCA") # two components before the elbow

### Continue with unstandardized PCA and 2 components according to scree plot
pc_scores <- predict(Data.pca, newdata = Data_pca, 2)[, 1:2]
```

Let's now implement the 5-fold cross-validation to assess what number of principal components.

```{r}
# Step 1: Set up cross-validation
set.seed(123)
folds <- createFolds(Data$disease, k = 5, list = TRUE)  # 5-fold CV

# Step 2: Cross-validation loop with PCA on training data
results <- data.frame(Components = numeric(),
                      Accuracy = numeric(),
                      AUC = numeric())

components_to_test <- seq(2, 33)  # Test up to 33 components

for (num_components in components_to_test) {
  metrics <- data.frame(Accuracy = numeric(), AUC = numeric())
  
  for (i in seq_along(folds)) {
    # Split data into training and testing folds
    train_idx <- unlist(folds[-i])
    test_idx <- folds[[i]]
    train_data <- Data[train_idx, ]
    test_data <- Data[test_idx, ]
    
    # Step 3: Perform PCA on training data
    train_pca <- train_data %>% dplyr::select(-disease) %>% as.matrix()
    test_pca <- test_data %>% dplyr::select(-disease) %>% as.matrix()
    
    pca_model <- prcomp(train_pca, center = FALSE, scale = FALSE)  # Fit PCA on training data
    
    # Step 4: Transform both training and testing data using the same PCA model 
    train_pcs <- pca_model$x[, 1:num_components]  # Select top N components for training data
    test_pcs <- predict(pca_model, newdata = test_pca)[, 1:num_components]  # Transform test data
    
    # Combine PCs with the outcome variable
    train_df <- data.frame(disease = train_data$disease, train_pcs)
    test_df <- data.frame(disease = test_data$disease, test_pcs)
    
    # Step 5: Fit logistic regression
    model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)
    
    # Step 6: Predict on the test set
    pred_probs <- predict(model, newdata = test_df, type = "response")
    pred_class <- ifelse(pred_probs > 0.5, 1, 0)
    
    # Calculate metrics
    accuracy <- mean(pred_class == test_df$disease)
    auc <- roc(as.factor(test_df$disease), pred_probs)$auc
    
    # Store metrics for the fold
    metrics <- rbind(metrics, data.frame(Accuracy = accuracy, AUC = auc))
  }
  
  # Average metrics across folds
  avg_accuracy <- mean(metrics$Accuracy)
  avg_auc <- mean(metrics$AUC)
  
  # Store results
  results <- rbind(results, data.frame(Components = num_components,
                                       Accuracy = avg_accuracy,
                                       AUC = avg_auc))
}

# Step 7: Find the optimal number of components based on AUC and Accuracy
optimal_components_AUC <- results %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy <- results %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# now filter first only components on PC smaller than or equal to 15
optimal_components_AUC_selected <- results %>% 
  filter(Components <= 15) %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy_selected <- results %>%
  filter(Components <= 15) %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# Step 8: Visualize results
ggplot(results, aes(x = Components)) +
  geom_line(aes(y = AUC, color = "AUC")) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  labs(title = "5-Fold Cross-Validation Results",
       x = "Number of Principal Components",
       y = "Metric") +
  scale_color_manual(values = c("AUC" = "blue", "Accuracy" = "red")) +
  theme_minimal()

# Print optimal number of components
cat("Optimal number of components based on AUC:", optimal_components_AUC, "\n")
cat("Optimal number of components based on Accuracy:", optimal_components_accuracy, "\n")
cat("Optimal number of components based on AUC (under 15 PC):", optimal_components_AUC_selected, "\n")
cat("Optimal number of components based on Accuracy (under 15 PC):", optimal_components_accuracy_selected, "\n")

# save results to rds
saveRDS(results, "results_pca.rds")
```

See what is recommended by the books that we use. In principal component regression, the number of components is selected by looking at the squared bias, test MSE and variance.

"In PCR, the number of principal components, M, is typically chosen by
cross-validation." (ISLR, p. 259)

> Compute cross-validation MSE, which is obtained using PCR as a function of the number of components. In logistic regression, however, we employ the AUC and accuracy as metrics to determine the optimal number of components.

# Method 3: Sparse PCA with logistic regression

The third approach implemented sparse PCA, using the \texttt{elasticnet} package's \texttt{spca()} function (Zou et al., 2006). Sparse PCA incorporates sparsity constraints \cite{hastie2015statistical} to ensure that each component is influenced by a subset of genes, enhancing interpretability while retaining predictive power. Hyperparameters controlling sparsity, such as the number of nonzero loadings, were tuned using cross-validation. Logistic regression was then applied to the sparse principal components using the \texttt{glm()} function.

Zou, H., Hastie, T., & Tibshirani, R. (2006). Sparse Principal Component Analysis. Journal of Computational and Graphical Statistics, 15(2), 265–286. https://doi.org/10.1198/106186006X113430

"We first show how PCA can be recast exactly in terms of a (ridge) regression problem. We then introduce the lasso penalty by changing this ridge regression to an elastic-net regression." (Zou et al., 2006)

> sparse PCA is to ordinary PCA is lasso regression is to ridge regression (Zou et al., 2006)

While sparse PCA can be conducted with the spca() function of the elasticnet package (Zou et al., 2006), a more recent implementation is available in the sparsepca package (Erichson et al., 2020). The sparsepca package employs varriable projection and offers an accelerated version of sparse PCA with an alpha parameter than controlls the sparsity and a beta parameter than determines the amount of ridge shrinkage.

Erichson, N. B., Zheng, P., Manohar, K., Brunton, S. L., Kutz, J. N., & Aravkin, A. Y. (2020). Sparse Principal Component Analysis via Variable Projection. SIAM Journal on Applied Mathematics, 80(2), 977–1002. https://doi.org/10.1137/18M1211350

let us first do it on the complete data

```{r}
library(sparsepca)

# Fit sparse PCA models
spca_model <- sparsepca::spca(Data_pca, k = 2, alpha = 1e-04, beta = 1e-04, center = FALSE, 
                              scale = FALSE, max_iter = 1000, tol = 1e-05, verbose = TRUE)

summary(spca_model)
spca_model$scores

# Or an accelerated implementation
spca_model_acc <- sparsepca::rspca(Data_pca, k = 2, alpha = 1e-04, beta = 1e-04, center = FALSE, 
                              scale = FALSE, max_iter = 1000, tol = 1e-05, verbose = TRUE)
```


Let's now implement the 5-fold cross-validation to determine the optimal number of components and values for the tuning parameters.

```{r}
# Step 1: Set up cross-validation
set.seed(123)
folds <- createFolds(Data$disease, k = 5, list = TRUE)  # 5-fold CV

# Step 2: Cross-validation loop with PCA on training data
results <- data.frame(Components = numeric(),
                      Accuracy = numeric(),
                      AUC = numeric())

components_to_test <- seq(2, 33)  # Test up to 33 components

for (num_components in components_to_test) {
  metrics <- data.frame(Accuracy = numeric(), AUC = numeric())
  
  for (i in seq_along(folds)) {
    # Split data into training and testing folds
    train_idx <- unlist(folds[-i])
    test_idx <- folds[[i]]
    train_data <- Data[train_idx, ]
    test_data <- Data[test_idx, ]
    
    # Step 3: Perform PCA on training data
    train_pca <- train_data %>% dplyr::select(-disease) %>% as.matrix()
    test_pca <- test_data %>% dplyr::select(-disease) %>% as.matrix()
    
    spca_model <- sparsepca::spca(train_pca, k = max(components_to_test), alpha = 1e-04, 
                                  beta = 1e-04, center = FALSE, scale = FALSE)
    
    # Step 4: Transform both training and testing data using the same PCA model 
    train_pcs <- spca_model$scores[, 1:num_components]  # Select top N components for training data
    test_pcs <- predict(pca_model, newdata = test_pca)[, 1:num_components]  # Transform test data
    
    # Combine PCs with the outcome variable
    train_df <- data.frame(disease = train_data$disease, train_pcs)
    test_df <- data.frame(disease = test_data$disease, test_pcs)
    
    # Step 5: Fit logistic regression
    model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)
    
    # Step 6: Predict on the test set
    pred_probs <- predict(model, newdata = test_df, type = "response")
    pred_class <- ifelse(pred_probs > 0.5, 1, 0)
    
    # Calculate metrics
    accuracy <- mean(pred_class == test_df$disease)
    auc <- roc(as.factor(test_df$disease), pred_probs)$auc
    
    # Store metrics for the fold
    metrics <- rbind(metrics, data.frame(Accuracy = accuracy, AUC = auc))
  }
  
  # Average metrics across folds
  avg_accuracy <- mean(metrics$Accuracy)
  avg_auc <- mean(metrics$AUC)
  
  # Store results
  results <- rbind(results, data.frame(Components = num_components,
                                       Accuracy = avg_accuracy,
                                       AUC = avg_auc))
}

# Step 7: Find the optimal number of components based on AUC and Accuracy
optimal_components_AUC <- results %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy <- results %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# now filter first only components on PC smaller than or equal to 15
optimal_components_AUC_selected <- results %>% 
  filter(Components <= 15) %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy_selected <- results %>%
  filter(Components <= 15) %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# Step 8: Visualize results
ggplot(results, aes(x = Components)) +
  geom_line(aes(y = AUC, color = "AUC")) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  labs(title = "5-Fold Cross-Validation Results",
       x = "Number of Principal Components",
       y = "Metric") +
  scale_color_manual(values = c("AUC" = "blue", "Accuracy" = "red")) +
  theme_minimal()

# Print optimal number of components
cat("Optimal number of components based on AUC:", optimal_components_AUC, "\n")
cat("Optimal number of components based on Accuracy:", optimal_components_accuracy, "\n")
cat("Optimal number of components based on AUC (under 15 PC):", optimal_components_AUC_selected, "\n")
cat("Optimal number of components based on Accuracy (under 15 PC):", optimal_components_accuracy_selected, "\n")

# save results to rds
saveRDS(results, "results_spca.rds")
```

```{r}
library(sparsepca)
library(caret)
library(pROC)

# Define hyperparameter grid to perform "grid search"
k_values <- seq(2, 11)  # Number of components to test
alpha_values <- c(1e-04, 1e-03, 1e-02, 1e-01)  # Sparsity penalty
beta_values <- c(1e-04, 1e-03, 1e-02, 1e-01)   # Variance penalty
# Thus, we have 4 * 4 * 10 = 160 combinations to test

set.seed(123)
folds <- createFolds(Data$disease, k = 5, list = TRUE)  # 5-fold CV

results <- data.frame(K = numeric(), Alpha = numeric(), Beta = numeric(),
                      Accuracy = numeric(), AUC = numeric())

for (i in seq_along(folds)) {
  # Split data into training and testing folds
  train_idx <- unlist(folds[-i])
  test_idx <- folds[[i]]
  train_data <- Data[train_idx, ]
  test_data <- Data[test_idx, ]
  
  train_pca <- train_data %>% dplyr::select(-disease) %>% as.matrix()
  test_pca <- test_data %>% dplyr::select(-disease) %>% as.matrix()
  
  # Grid search for k, alpha, and beta
  for (k in k_values) {
    for (alpha in alpha_values) {
      for (beta in beta_values) {
        # Fit SPCA with current hyperparameters
        spca_model <- sparsepca::rspca(train_pca, k = k,
                                      alpha = alpha, beta = beta,
                                      center = FALSE, scale = FALSE)
        
        # Transform both training and testing data
        train_pcs <- spca_model$scores
        
        # Get the loadings from the trained sparse PCA model
        loadings <- spca_model$loadings  # (p, k)
        
        # Center the test data based on the training data's mean (as SPCA is centering based)
        test_pca_centered <- sweep(test_pca, 2, colMeans(train_pca), FUN = "-")
        
        # Project test data onto the sparse principal components (using the loadings)
        test_pcs <- test_pca_centered %*% loadings  # (n, k)
        
        # Combine PCs with the outcome variable
        train_df <- data.frame(disease = train_data$disease, train_pcs)
        test_df <- data.frame(disease = test_data$disease, test_pcs)
        
        # Fit logistic regression
        model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)
        
        # Predict on the test set
        pred_probs <- predict(model, newdata = test_df, type = "response")
        pred_class <- ifelse(pred_probs > 0.5, 1, 0)
        
        # Calculate metrics
        accuracy <- mean(pred_class == test_df$disease)
        auc <- roc(as.factor(test_df$disease), pred_probs)$auc
        
        # Store results
        results <- rbind(results, data.frame(K = k, Alpha = alpha, Beta = beta,
                                             Accuracy = accuracy, AUC = auc))
      }
    }
  }
}

# save results to rds
saveRDS(results, "results_spca_hyperparameters.rds")
```

Let's assess the results

```{r}
results <- readRDS("results_spca_hyperparameters.rds")
# results

# Find the optimal number of components based on AUC and Accuracy
best_AUC <- results[which.max(results$AUC), ]
print(best_AUC) # K = 7,	Alpha = 1e-04, Beta =	1e-04,	Accuracy = 0.950, AUC =	1.000
best_accuracy <- results[which.max(results$Accuracy), ]
print(best_accuracy) # K = 11,	Alpha = 1e-04, Beta =	0.001, Accuracy = 0.952, AUC = 0.898

### Visualize results

# Create a line plot for AUC and accuracy as a function of the number of components
ggplot(results, aes(x = K)) +
  geom_line(aes(y = AUC, color = "AUC")) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  labs(title = "5-Fold Cross-Validation Results",
       x = "Number of Principal Components",
       y = "Metric") +
  scale_color_manual(values = c("AUC" = "blue", "Accuracy" = "red")) +
  theme_minimal()

library(plotly)

# Create a 3D scatter plot
fig <- plot_ly(results, x = ~K, y = ~Alpha, z = ~Beta, color = ~AUC, 
              colors = c("blue", "red"), type = "scatter3d", mode = "markers") %>%
  layout(title = "AUC for Different Hyperparameters (k, alpha, beta)",
         scene = list(xaxis = list(title = "Number of Components (k)"),
                      yaxis = list(title = "Sparsity Penalty (alpha)"),
                      zaxis = list(title = "Variance Penalty (beta)")))
fig
```





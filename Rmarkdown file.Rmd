---
title: "R markdown file"
author: "Floris Meijvis en co"
date: "2024-11-22"
output: pdf_document
---

Importing packages:

```{r} 
library(ggplot2)
library(tidyverse)
library(spls)
library(tensorflow)
library(caret)
library(caret)
library(pROC)
library(pls)
library(caret)     # For cross-validation
library(pROC)      # For AUC calculation
library(dplyr)     # For data manipulation
```

# Importing and Mutating Data

-> problem?! gene names are gone in Data dataframe

```{r}
# import and mutate Data
data(prostate)
x <- prostate$x
y <- prostate$y

Data <- data.frame(y, x) %>% 
  # rename column y to tumor
  rename(disease = y)

# explore data
nrow(Data) #102 samples -> 52 tumor samples, 50 normal tissue samples
ncol(Data) #6034 genes included in data set
all(sapply(Data, is.numeric)) # TRUE, all variables are operationalized as numeric
head(Data, 10)
```

The prostate dataset consists of 52 prostate tumor and 50 normal samples. Normal and tumor classes are coded in 0 and 1, respectively, in $Y$ vector. Matrix $X$ is gene expression data and arrays were normalized, log transformed, and standardized to zero mean and unit variance across genes as described in Dettling (2004) and Dettling and Beuhlmann (2002). See Chung and Keles (2010) for more details.

https://rdrr.io/cran/spls/man/prostate.html

# Data visualization

```{r}
### making a few visualizations of the data

# boxplot for possible tumor promoting gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X54, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X54", title = "Expression level of gene X54 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))


# boxplot for possible tumor supressing gene
Data %>%
  mutate(disease = ifelse(disease == 0, "normal", "tumor")) %>%
  ggplot(aes(y = X2022, fill = disease)) + 
  geom_boxplot() +
  labs(y = "Gene expression of gen X2022", title = "Expression level of gene X2022 in normal vs tumor sample") +
  theme(axis.text.x = element_blank()) + 
  facet_wrap(vars(disease))
```

A few quickly made observations: it seems that the tumors have less variance in the gene expressions (but this is based only on 3 plots, so could be wrong)

```{r}
# compute the average sd in gene expressions for normal and tumor samples
normal <- Data %>% filter(disease == 0) %>% dplyr::select(-disease)
tumor <- Data %>% filter(disease == 1) %>% dplyr::select(-disease)
normal_sd <- apply(normal, 2, sd)
tumor_sd <- apply(tumor, 2, sd)

data_sd <- data.frame(normal = normal_sd, tumor = tumor_sd) %>%
  pivot_longer(cols = everything(), names_to = "disease", values_to = "sd")

# visualize the average sd in gene expressions for normal and tumor samples using boxplot
ggplot(data_sd, aes(x = disease, y = sd, fill = disease)) +
  geom_boxplot() +
  labs(y = "Average standard deviation in gene expressions", title = "Average standard deviation in gene expressions for normal and tumor samples")
```

There does not seem to be massive variation in the standard deviations of gene expressions between normal and tumor samples. However, we do generally see that the standard deviations are lower in the tumor samples compared to the normal samples, especially since there are less high outliers for the tumor samples.

# Method 1: Hybrid step-wise Logistic regression

The first approach employed hybrid stepwise logistic regression, implemented using the \texttt{MASS} package’s \texttt{stepAIC()} function. This method iteratively selects predictors by minimizing the Akaike Information Criterion (AIC) through a combination of forward selection and backward elimination \cite{hastie2009elements}. Alternatively, a more rigorous but computationally expensive strategy was also considered: removing one variable at a time and performing an analysis of deviance using differences in residual deviance to identify the least important variable to exclude \cite{hastie2009elements}. 

```{r}
## TRAIN / TEST DATA SPLIT
# divide data in train/test data split (80/20%)

# function ensures that the proportion of the outcome variable is preserved in both sets.
set.seed(123)
training.samples <- Data$disease %>%
  createDataPartition(p = 0.8, list = FALSE) 
train.data <- Data[training.samples, ]
test.data <- Data[-training.samples, ]
dim(train.data)
dim(test.data)

# # Optional: Automatically add quadratic terms for all predictors
# predictors <- colnames(train.data)[-which(colnames(train.data) == "disease")]
# quadratic_data <- train.data
# for (pred in predictors) {
#   quadratic_data[[paste0(pred, "_sq")]] <- train.data[[pred]]^2
# }
# 
# # Update the dataset
# train.data <- quadratic_data

# fit logistic regression models
intercept.model <- glm(disease ~ 1, family = "binomial", data = train.data)
full.model <- glm(disease ~ ., family = "binomial", data = train.data) 
summary(full.model)

# probit.model <- glm(disease ~ ., family = binomial(link = "probit"), data = train.data) # no parameter significant
# summary(probit.model)
# cloglog.model <- glm(disease ~ ., family = binomial(link = "cloglog"), data = train.data) # no parameter significant
# summary(cloglog.model)
# identity.model <- glm(disease ~ ., family = binomial(link = "identity"), data = train.data) # "no valid set of coefficients has been found: please supply starting values"
# there is a clear issue of multicollinearity 
# warning "5952 not defined because of singularities"

# Perform stepwise logistic regression
library(MASS)
step.model <- stepAIC(object = intercept.model, direction = "both", 
                      scope = full.model, trace = FALSE)

# step.model2 <- stepAIC(object = intercept.model, direction = "backward", 
#                       scope = full.model, trace = FALSE)
# 
# step.model3 <- stepAIC(object = intercept.model, direction = "forward", 
#                       scope = full.model, trace = FALSE)

# Predict probabilities and classify on validation set
predictions <- predict(step.model, newdata = test.data, type = "response") # all predictions are identical as the intercept is the only parameter in the final model
pred_class <- ifelse(predictions > 0.5, 1, 0) # and since the intercept is slightly greater then 0.5, all predictions are 1

# Compute confusion matrix
cm <- confusionMatrix(factor(pred_class), factor(test.data$disease), positive = "1")
cm
  
# Compute AUC
roc_curve <- roc(factor(test.data$disease), predictions)
auc(roc_curve)

# draw ROC curve
plot(roc_curve, col = "blue", lwd = 2, main = "ROC curve for logistic regression model")
```

Let's now do it with 5-fold cross-validation

```{r}
# Define 5-fold cross-validation
set.seed(123)
folds <- createFolds(train.data$disease, k = 5, list = TRUE, returnTrain = TRUE)

# Initialize vectors to store performance metrics
accuracy <- c()
sensitivity <- c()
specificity <- c()
auc <- c()

# Perform 5-fold CV
for (i in 1:5) {
  # Split the data into training and validation sets
  fold_train <- train.data[folds[[i]], ]
  fold_test <- train.data[-folds[[i]], ]
  
  # Fit intercept and full models
  intercept.model <- glm(disease ~ 1, family = "binomial", data = fold_train)
  full.model <- glm(disease ~ ., family = "binomial", data = fold_train)
  
  # Perform stepwise logistic regression
  step.model <- stepAIC(intercept.model, direction = "both", 
                        scope = full.model, trace = FALSE)
  
  # Predict probabilities and classify on validation set
  predictions <- predict(step.model, newdata = fold_test, type = "response")
  pred_class <- ifelse(predictions > 0.5, 1, 0)
  
  # Compute confusion matrix
  cm <- confusionMatrix(factor(pred_class), factor(fold_test$disease), positive = "1")
  
  # Compute metrics
  accuracy[i] <- cm$overall["Accuracy"]
  sensitivity[i] <- cm$byClass["Sensitivity"]
  specificity[i] <- cm$byClass["Specificity"]
  
  # Compute AUC
  roc_curve <- roc(factor(fold_test$disease), predictions)
  auc[i] <- auc(roc_curve)
}

# Print the results
cat("5-Fold Cross-Validation Results:\n")
cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
cat("AUC:", auc, "\n")

# the accuracy is between 0.375 and 0.588, which is not very good
# either the sensitivity or specificity is 1, there is no in-between, indicating that the model
# is either good at predicting the positive class or the negative class, but not both.
# the AUC is consistent at 0.5, which is the same as random guessing

# Calculate average metrics
avg_accuracy <- mean(accuracy)
avg_sensitivity <- mean(sensitivity)
avg_specificity <- mean(specificity)
avg_auc <- mean(auc)
```

# Method 2: Principle component analysis (PCA) with logistic regression

The second approach utilized Principal Component Analysis (PCA), performed with the \texttt{stats} package’s \texttt{prcomp()} function. PCA transforms the original 6,024 predictors into orthogonal components that capture the majority of variance in the dataset. Components explaining a cumulative variance of 95\% were retained and used as predictors in logistic regression, implemented using the \texttt{glm()} function. While PCA effectively reduces dimensionality, it does not enforce sparsity, potentially limiting \textit{biological interpretability} \citep{hastie2015statistical}.

```{r}
# remove the outcome
Data_pca <- Data %>% dplyr::select(-disease)

# summary(Data_pca[,1:30])
Data_pca <- as.matrix(Data_pca)

Data.pca = prcomp(Data_pca,
                  center = FALSE, # arrays were already normalized and centered
                  scale = FALSE)

Data.pca.stand = prcomp(Data_pca,
                  center = TRUE, # sensitivity analysis
                  scale = TRUE)

# evaluate cumulative variance
summary(Data.pca) 
which(cumsum(Data.pca$sdev^2) / sum(Data.pca$sdev^2) >= 0.95)[1] # 33 components are needed to explain 95% of the variance
summary(Data.pca.stand) 
which(cumsum(Data.pca.stand$sdev^2) / sum(Data.pca.stand$sdev^2) >= 0.95)[1] # 67 components are needed to explain 95% of the variance
# either lower threshold (e.g., 0.90 or use scree plot to determine number of components)

# make scree plot
screeplot(Data.pca, type = "lines", main = "Scree plot of PCA") # two components before the elbow
screeplot(Data.pca.stand, type = "lines", main = "Scree plot of PCA standardized") # one component before the elbow

### Continue with unstandardized PCA and 2 components according to scree plot
pc_scores <- predict(Data.pca, newdata = Data_pca, 2)[, 1:2]
```

Let's now implement the 5-fold cross-validation to assess what number of principal components.

```{r}
# Step 1: Set up cross-validation
set.seed(123)
folds <- createFolds(Data$disease, k = 5, list = TRUE)  # 5-fold CV

# Step 2: Cross-validation loop with PCA on training data
results <- data.frame(Components = numeric(),
                      Accuracy = numeric(),
                      AUC = numeric())

components_to_test <- seq(2, 33)  # Test up to 33 components

for (num_components in components_to_test) {
  metrics <- data.frame(Accuracy = numeric(), AUC = numeric())
  
  for (i in seq_along(folds)) {
    # Split data into training and testing folds
    train_idx <- unlist(folds[-i])
    test_idx <- folds[[i]]
    train_data <- Data[train_idx, ]
    test_data <- Data[test_idx, ]
    
    # Step 3: Perform PCA on training data
    train_pca <- train_data %>% dplyr::select(-disease) %>% as.matrix()
    test_pca <- test_data %>% dplyr::select(-disease) %>% as.matrix()
    
    pca_model <- prcomp(train_pca, center = FALSE, scale = FALSE)  # Fit PCA on training data
    
    # Step 4: Transform both training and testing data using the same PCA model 
    train_pcs <- pca_model$x[, 1:num_components]  # Select top N components for training data
    test_pcs <- predict(pca_model, newdata = test_pca)[, 1:num_components]  # Transform test data
    
    # Combine PCs with the outcome variable
    train_df <- data.frame(disease = train_data$disease, train_pcs)
    test_df <- data.frame(disease = test_data$disease, test_pcs)
    
    # Step 5: Fit logistic regression
    model <- glm(as.factor(disease) ~ ., family = "binomial", data = train_df)
    
    # Step 6: Predict on the test set
    pred_probs <- predict(model, newdata = test_df, type = "response")
    pred_class <- ifelse(pred_probs > 0.5, 1, 0)
    
    # Calculate metrics
    accuracy <- mean(pred_class == test_df$disease)
    auc <- roc(as.factor(test_df$disease), pred_probs)$auc
    
    # Store metrics for the fold
    metrics <- rbind(metrics, data.frame(Accuracy = accuracy, AUC = auc))
  }
  
  # Average metrics across folds
  avg_accuracy <- mean(metrics$Accuracy)
  avg_auc <- mean(metrics$AUC)
  
  # Store results
  results <- rbind(results, data.frame(Components = num_components,
                                       Accuracy = avg_accuracy,
                                       AUC = avg_auc))
}

# Step 7: Find the optimal number of components based on AUC and Accuracy
optimal_components_AUC <- results %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy <- results %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# now filter first only components on PC smaller than or equal to 15
optimal_components_AUC_selected <- results %>% 
  filter(Components <= 15) %>% 
  filter(AUC == max(AUC)) %>% 
  dplyr::select(Components) %>% 
  pull()

optimal_components_accuracy_selected <- results %>%
  filter(Components <= 15) %>% 
  filter(Accuracy == max(Accuracy)) %>% 
  dplyr::select(Components) %>% 
  pull()

# Step 8: Visualize results
ggplot(results, aes(x = Components)) +
  geom_line(aes(y = AUC, color = "AUC")) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  labs(title = "5-Fold Cross-Validation Results",
       x = "Number of Principal Components",
       y = "Metric") +
  scale_color_manual(values = c("AUC" = "blue", "Accuracy" = "red")) +
  theme_minimal()

# Print optimal number of components
cat("Optimal number of components based on AUC:", optimal_components_AUC, "\n")
cat("Optimal number of components based on Accuracy:", optimal_components_accuracy, "\n")
cat("Optimal number of components based on AUC (under 15 PC):", optimal_components_AUC_selected, "\n")
cat("Optimal number of components based on Accuracy (under 15 PC):", optimal_components_accuracy_selected, "\n")

# save results to rds
saveRDS(results, "results_pca.rds")
```

See what is recommended by the books that we use. In principal component regression, the number of components is selected by looking at the squared bias, test MSE and variance.

"We note that even though PCR provides a simple way to perform regression using M <p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features" (ISLR, p. 259)

"In PCR, the number of principal components, M, is typically chosen by
cross-validation." (ISLR, p. 259)

> Compute cross-validation MSE, which is obtained using PCR as a function of the number of components.

"However, if the variables are all measured in the same units (say, kilograms, or inches), then one might choose not to standardize them"

> we choose not to standardize them, because the relative gen expression is meaningful and centering within each gene would result in only the within gene variance being used for the PCA.

# Method 3: Sparse PCA with logistic regression

The third approach implemented sparse PCA, using the \texttt{elasticnet} package's \texttt{spca()} function. Sparse PCA incorporates sparsity constraints \cite{hastie2015statistical} to ensure that each component is influenced by a subset of genes, enhancing interpretability while retaining predictive power. Hyperparameters controlling sparsity, such as the number of nonzero loadings, were tuned using cross-validation. Logistic regression was then applied to the sparse principal components using the \texttt{glm()} function.

let us first do it on the complete data

```{r}
library(elasticnet)
?spca()

# Fit sparse PCA models
spca_model_pen <- spca(Data_pca, K = 2, type = "predictor", sparse = "penalty", trace = TRUE)
spca_model_varnum <- spca(Data_pca, K = 2, type = "predictor", sparse = "varnum", trace = TRUE)

```

Let's now implement the 5-fold cross-validation to determine the optimal number of components and values for the tuning parameters.


